{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2334845-a4d5-4db6-be29-8ac68c55d2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stakenaka/go/src/github.com/ks6088ts-labs/handson-langchain/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-10 16:32:12.125375: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-10 16:32:12.166795: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-10 16:32:12.336366: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-10 16:32:12.336410: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-10 16:32:12.337119: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-10 16:32:12.430820: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-10 16:32:12.432638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-10 16:32:13.493644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "2023-10-10 16:32:14.961283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:f3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-10 16:32:14.962952: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 感情分析\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# 推論\n",
    "print(nlp(\"I love you\")) # ポジティブ\n",
    "print(nlp(\"I hate you\")) # ネガティブ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065835d6-7ac6-4f8b-b1b4-d4bd6180de05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6222442388534546, 'start': 34, 'end': 95, 'answer': 'the task of extracting an answer from a text given a question'}\n",
      "{'score': 0.511530339717865, 'start': 147, 'end': 160, 'answer': 'SQuAD dataset'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 質疑応答\n",
    "nlp = pipeline(\"question-answering\")\n",
    "\n",
    "# テキスト\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the `run_squad.py`.\n",
    "\"\"\"\n",
    "# 質問回答とは、与えられたtテキストから質問の回答を抽出するタスクです。\n",
    "# データセットの例は、「SQuAD」です。\n",
    "# ファインチューニングしたい場合は「run_squad.py」を利用できます。\n",
    "\n",
    "# 推論\n",
    "print(nlp(question=\"What is extractive question answering?\", context=context))\n",
    "print(nlp(question=\"What is a good example of a question answering dataset?\", context=context))\n",
    "# 質問回答とは？\n",
    "# データセットの例は？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20dcc40-64d5-4b2f-8684-3f9c07ab5222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.17927435040473938, 'token': 3944, 'token_str': ' tool', 'sequence': 'HuggingFace is creating a tool that the community uses to solve NLP tasks.'}, {'score': 0.11349397152662277, 'token': 7208, 'token_str': ' framework', 'sequence': 'HuggingFace is creating a framework that the community uses to solve NLP tasks.'}, {'score': 0.052435025572776794, 'token': 5560, 'token_str': ' library', 'sequence': 'HuggingFace is creating a library that the community uses to solve NLP tasks.'}, {'score': 0.03493502363562584, 'token': 8503, 'token_str': ' database', 'sequence': 'HuggingFace is creating a database that the community uses to solve NLP tasks.'}, {'score': 0.028602302074432373, 'token': 17715, 'token_str': ' prototype', 'sequence': 'HuggingFace is creating a prototype that the community uses to solve NLP tasks.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Masked Lanuage Modeling\n",
    "nlp = pipeline(\"fill-mask\")\n",
    "\n",
    "# 推論\n",
    "print(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))\n",
    "# HuggingFaceは、コミュニティがNLPのタスクを解くために使う{nlp.tokenizer.mask_token}を作成しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20009687-1e81-45d8-a43b-941fea864f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'As far as I am concerned, I will say that I am completely behind the people who have brought this issue up. Many of those around the group have stated that they understand and will take any help they have to get things done as quickly as possible'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# テキスト生成\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "\n",
    "# 推論\n",
    "print(text_generator(\"As far as I am concerned, I will\", max_length=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79af1795-cb30-4db1-b077-6c02c2103c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-small and revision d769bba (https://huggingface.co/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "2023-10-10 16:32:35.690036: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ef6bd6e150 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-10 16:32:35.690079: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-10-10 16:32:35.710009: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-10 16:32:35.781604: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-10-10 16:32:35.782463: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'in 2010, she married once more, this time in the Bronx . she is facing two criminal counts of \"offering a false instrument for filing in the first degree\" the marriages were part of an immigration scam .'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 要約\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# テキスト\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "\n",
    "# 推論\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878494fc-2224-4350-afec-49158d585c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "/home/stakenaka/go/src/github.com/ks6088ts-labs/handson-langchain/.venv/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Hugging Face ist ein Technologieunternehmen mit Sitz in New York und Paris.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 翻訳\n",
    "translator = pipeline(\"translation_en_to_de\")\n",
    "\n",
    "# 推論\n",
    "print(translator(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc50b278-e91a-45b1-9d5a-c5026f80dfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/stakenaka/.cache/huggingface/hub/models--google--pegasus-xsum/snapshots/8d8ffc158a3bee9fbb03afacdfc347c823c5ec8b/config.json'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"google/pegasus-xsum\", filename=\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e9d92d-ad24-4c7b-9c5b-ead48e5f6bcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_lzma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Print all the available datasets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_datasets\n",
      "File \u001b[0;32m~/go/src/github.com/ks6088ts-labs/handson-langchain/.venv/lib/python3.10/site-packages/datasets/__init__.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position\u001b[39;00m\n\u001b[1;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.14.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/go/src/github.com/ks6088ts-labs/handson-langchain/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:66\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n",
      "File \u001b[0;32m~/go/src/github.com/ks6088ts-labs/handson-langchain/.venv/lib/python3.10/site-packages/datasets/arrow_reader.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryTable, MemoryMappedTable, Table, concat_tables\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_path\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/go/src/github.com/ks6088ts-labs/handson-langchain/.venv/lib/python3.10/site-packages/datasets/utils/file_utils.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextract\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExtractManager\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilelock\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileLock\n\u001b[1;32m     38\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[0;32m~/go/src/github.com/ks6088ts-labs/handson-langchain/.venv/lib/python3.10/site-packages/datasets/utils/extract.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlzma\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/lzma.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_lzma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_lzma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _encode_filter_properties, _decode_filter_properties\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_compression\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_lzma'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Print all the available datasets\n",
    "from huggingface_hub import list_datasets\n",
    "print([dataset.id for dataset in list_datasets()])\n",
    "\n",
    "# Load a dataset and print the first example in the training set\n",
    "squad_dataset = load_dataset('squad')\n",
    "print(squad_dataset['train'][0])\n",
    "\n",
    "# Process the dataset - add a column with the length of the context texts\n",
    "dataset_with_length = squad_dataset.map(lambda x: {\"length\": len(x[\"context\"])})\n",
    "\n",
    "# Process the dataset - tokenize the context texts (using a tokenizer from the 🤗 Transformers library)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "tokenized_dataset = squad_dataset.map(lambda x: tokenizer(x['context']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a164db-f195-4418-afac-d8301e751d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
